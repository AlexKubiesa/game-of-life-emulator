{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training cell predictor\n",
    "\n",
    "In this experiment, we will try to improve the inference speed of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below predict the next state of a cell explicitly. The timing results show that each prediction takes about 7.5 microseconds. Performing this for each cell in a 50 x 50 grid would take 7.5 * 50**2 = 18750 microseconds ~= 19 milliseconds ~= 0.02 seconds. This is barely noticeable when the time between display.flip calls is 0.1 seconds and doesn't impact it too much when the time between display.flip calls is 0.01 seconds.\n",
    "\n",
    "Let's compare this to model-based prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "cell_states = np.array([[1, 0, 0], [0, 0, 0], [1, 0, 1]])\n",
    "_neighbour_mask = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "\n",
    "alive = cell_states[1, 1] == 1\n",
    "alive_neighbours = np.sum(cell_states * _neighbour_mask)\n",
    "a = (\n",
    "    int(alive_neighbours in [2, 3])\n",
    "    if alive\n",
    "    else int(alive_neighbours == 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellPredictorNeuralNetwork(nn.Module):\n",
    "    \"\"\"Predicts the next state of the cells.\n",
    "\n",
    "    Inputs:\n",
    "        x: Tensor of shape (batch_size, channels, width, height), where channels=1, width=3 and height=3.\n",
    "    \n",
    "    Returns: Tensor of shape (batch_size,), the logits of the predicted states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, 85, 3)\n",
    "        self.linear1 = nn.Linear(85, 10)\n",
    "        self.linear2 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        logits = self.linear2(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CellPredictorNeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the loaded model takes about 130 microseconds to make each prediction - over 20 times longer than the rule-based method. Performing this for each cell in a 50 x 50 grid would take 130 * 50**2 = 325000 microseconds = 325 milliseconds ~= 0.3 seconds. This is larger than the time between display.flip calls, which is likely why the difference between model-based prediction and rule-based prediction is so noticeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "cell_states = np.array([[1, 0, 0], [0, 0, 0], [1, 0, 1]])\n",
    "# Convert to torch tensor, add batch and channel dimensions\n",
    "X = torch.tensor(cell_states, dtype=torch.float).expand((1, 1, -1, -1))\n",
    "with torch.no_grad():\n",
    "    logit = model(X).item()\n",
    "result = int(logit > 0.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to time models effectively"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the model need to be trained to be timed accurately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CellPredictorNeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "cell_states = np.array([[1, 0, 0], [0, 0, 0], [1, 0, 1]])\n",
    "# Convert to torch tensor, add batch and channel dimensions\n",
    "X = torch.tensor(cell_states, dtype=torch.float).expand((1, 1, -1, -1))\n",
    "with torch.no_grad():\n",
    "    logit = model(X).item()\n",
    "result = int(logit > 0.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like an untrained model takes about the same amount of time to run as a trained model. So, we can try different model architectures without training them to compare inference times."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the data need to be representative or do all examples take the same amount of time to process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameOfLifeDataset(torch.utils.data.Dataset):\n",
    "    _size = 512\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 12 -> \"0b1100\" -> \"1100\" -> \"000001100\"\n",
    "        idx_bin = bin(idx)[2:].rjust(9, \"0\")\n",
    "        X = torch.tensor([float(ch) for ch in idx_bin], dtype=torch.float32).reshape(1, 3, 3) # (channels, width, height)\n",
    "        alive = X[0, 1, 1] > 0.5\n",
    "        alive_neighbours = torch.sum(X) - X[0, 1, 1]\n",
    "        next_alive = (alive and alive_neighbours > 1.5 and alive_neighbours < 3.5) or (not alive and alive_neighbours > 2.5 and alive_neighbours < 3.5)\n",
    "        y = torch.tensor([float(next_alive)], dtype=torch.float32)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GameOfLifeDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True) # We use batch size 1 to be representative of inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def time_model(model, dataloader):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    time_taken = 0\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        tic = time.time()\n",
    "        with torch.no_grad():\n",
    "            pred = model(X)\n",
    "        toc = time.time()\n",
    "        time_taken += toc - tic\n",
    "    \n",
    "    avg_time = time_taken / num_batches\n",
    "    return avg_time\n",
    "\n",
    "times = []\n",
    "for i in range(100):\n",
    "    times.append(1_000_000 * time_model(model, dataloader))\n",
    "avg_time = np.mean(times)\n",
    "std_time = np.std(times)\n",
    "print(f\"Average time: {avg_time:.2f} microseconds, std: {std_time:.2f} microseconds.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that inference time for a random example is roughly the same as the inference time for the specific example picked earlier. The times aren't exactly the same, but that's likely because of the different methodology between our custom timing code and the %%timeit command."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance improvements\n",
    "\n",
    "Now let's experiment with some performance improvements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole-grid prediction\n",
    "\n",
    "Grid prediction could be done all at once by a fully-convolutional model, instead of one cell at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConvolutionalCellPredictor(nn.Module):\n",
    "    \"\"\"Predicts the next state of the cells.\n",
    "\n",
    "    Inputs:\n",
    "        x: Tensor of shape (batch_size, channels, width+2, height+2), where channels=1. width and height are the dimensions of the entire game grid.\n",
    "           We add one cell of padding on each side to ensure that predictions can be made for the boundary cells.\n",
    "    \n",
    "    Returns: Tensor of shape (batch_size, width, height), the logits of the predicted states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(1, 85, 3)\n",
    "        self.conv1 = nn.Conv2d(85, 10, 1)\n",
    "        self.conv2 = nn.Conv2d(10, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv0(x))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        logits = self.conv2(x)\n",
    "        logits = torch.squeeze(logits, 1) # Remove channels dimension\n",
    "        return logits\n",
    "\n",
    "fc_model = FullyConvolutionalCellPredictor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we time this model? We need some data that looks like an entire grid, plus boundary cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fc_model_input(size=50):\n",
    "    with torch.no_grad():\n",
    "        X = torch.randint(0, 2, (size, size), dtype=torch.float)\n",
    "        X = F.pad(X, (1, 1, 1, 1))\n",
    "        X = X.unsqueeze(0).unsqueeze(0) # Insert channels and batch dimensions\n",
    "    return X\n",
    "\n",
    "X = make_fc_model_input()\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = make_fc_model_input(50)\n",
    "%timeit fc_model(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fully-convolutional model takes about 590 µs to make predictions on the whole grid, which is much faster (over 500 times faster!) than the 130 * 50**2 = 325,000 µs taken by running the single-cell model on the whole grid one cell at a time.\n",
    "\n",
    "Varying the size (= width = height) of the grid to different powers of two gives the following data:\n",
    "\n",
    "| Grid size | Inference time (µs) |\n",
    "| --------- | ------------------- |\n",
    "| 1 | 135 |\n",
    "| 2 | 141 |\n",
    "| 4 | 148 |\n",
    "| 8 | 150 |\n",
    "| 16 | 291 |\n",
    "| 32 | 375 |\n",
    "| 64 | 1900 |\n",
    "| 128 | 6770 |\n",
    "| 256 | 32200 |\n",
    "| 512 | 121000 |\n",
    "| 1024 | 478000 |\n",
    "| 2048 | 1890000 |\n",
    "\n",
    "Plotting this data on a log-log plot shows a relationship that is roughly $ \\text{Time} = O((\\text{Grid size})^{1.12}) $. This is much better than single-cell prediction, which must be at least $ O((\\text{Grid size})^{2}) $, because iit requires the model to be run separately on each cell."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batched prediction\n",
    "\n",
    "Instead of predicting one cell at a time, we can feed a batch of cells into the model and get the predictions all at once.\n",
    "\n",
    "To make use of this, the game code wrapping this model would have to be updated so it batches up the cells and then reads the results out from the batch. This extra preprocessing and post-processing may introduce additional overhead and would be fiddly to code. Therefore, it seems most sensible to focus on the fully-convolutional approach for now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a fully-convolutional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to use a slightly different dataset because the model outputs tensors with width and height dimensions. The target values from this dataset will have\n",
    "# shape (channels, width, height), where channels = 1.\n",
    "\n",
    "class FullyConvolutionalDataset(torch.utils.data.Dataset):\n",
    "    _size = 512\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 12 -> \"0b1100\" -> \"1100\" -> \"000001100\"\n",
    "        idx_bin = bin(idx)[2:].rjust(9, \"0\")\n",
    "        X = torch.tensor([float(ch) for ch in idx_bin], dtype=torch.float32).reshape(1, 3, 3) # (channels, width, height)\n",
    "        alive = X[0, 1, 1] > 0.5\n",
    "        alive_neighbours = torch.sum(X) - X[0, 1, 1]\n",
    "        next_alive = (alive and alive_neighbours > 1.5 and alive_neighbours < 3.5) or (not alive and alive_neighbours > 2.5 and alive_neighbours < 3.5)\n",
    "        y = torch.tensor([float(next_alive)], dtype=torch.float32).reshape(1, 1) # (width, height)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FullyConvolutionalDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullyConvolutionalCellPredictor().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(dataloader))\n",
    "logits = model(X)[0]\n",
    "pred_probab = F.sigmoid(logits)\n",
    "y_pred = int(pred_probab > 0.5)\n",
    "print(f\"Predicted state: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        if batch % 20 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    avg_loss /= size\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            prob = F.sigmoid(pred)\n",
    "            correct += ((prob > 0.5).type(torch.float) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return {\n",
    "        \"loss\": test_loss,\n",
    "        \"acc\": correct\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "batch_size = 4\n",
    "epochs = 20\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_history = []\n",
    "test_loss_history = []\n",
    "test_acc_history = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    # Since our dataset represents the full set of possible states, we can safely use it for both training and testing\n",
    "    epoch_loss = train_loop(dataloader, model, loss_fn, optimizer)\n",
    "    loss_history.append(epoch_loss)\n",
    "    test_metrics = test_loop(dataloader, model, loss_fn)\n",
    "    test_loss_history.append(test_metrics[\"loss\"])\n",
    "    test_acc_history.append(test_metrics[\"acc\"])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8), constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "ax_loss, ax_test_loss, ax_test_acc, ax_blank = axs\n",
    "\n",
    "ax_loss.plot(loss_history)\n",
    "ax_loss.set_title(\"Training loss\")\n",
    "ax_loss.set_xlabel(\"epoch\")\n",
    "ax_loss.set_ylabel(\"loss\")\n",
    "\n",
    "ax_test_loss.plot(test_loss_history)\n",
    "ax_test_loss.set_title(\"Test loss\")\n",
    "ax_test_loss.set_xlabel(\"epoch\")\n",
    "ax_test_loss.set_ylabel(\"loss\")\n",
    "\n",
    "ax_test_acc.plot(test_acc_history)\n",
    "ax_test_acc.set_title(\"Test accuracy\")\n",
    "ax_test_acc.set_xlabel(\"epoch\")\n",
    "ax_test_acc.set_ylabel(\"accuracy\")\n",
    "\n",
    "ax_blank.axis(\"off\")\n",
    "\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
