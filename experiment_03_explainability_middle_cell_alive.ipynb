{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability\n",
    "\n",
    "In this experiment, we will try and use explainability techniques to understand what the model has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellPredictorNeuralNetwork(nn.Module):\n",
    "    \"\"\"Predicts the next state of the cells.\n",
    "\n",
    "    Inputs:\n",
    "        x: Tensor of shape (batch_size, channels, width+2, height+2), where channels=1. width and height are the dimensions of the entire game grid.\n",
    "           We add one cell of padding on each side to ensure that predictions can be made for the boundary cells.\n",
    "    \n",
    "    Returns: Tensor of shape (batch_size, width, height), the logits of the predicted states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(1, 85, 3)\n",
    "        self.conv1 = nn.Conv2d(85, 10, 1)\n",
    "        self.conv2 = nn.Conv2d(10, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv0(x))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        logits = self.conv2(x)\n",
    "        logits = torch.squeeze(logits, 1) # Remove channels dimension\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CellPredictorNeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model_weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1964)\n"
     ]
    }
   ],
   "source": [
    "# Generate activation for one feature and one example\n",
    "\n",
    "feature = 3\n",
    "\n",
    "X = torch.tensor([[0, 0, 0], [0, 1, 0], [0, 0, 0]], dtype=torch.float).reshape(1, 1, 3, 3)\n",
    "with torch.no_grad():\n",
    "    a = model.conv0(X)\n",
    "a = a.squeeze()[feature]\n",
    "print(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Middle cell alive or dead\n",
    "\n",
    "Here, we'll see how well the network has learned the concept of the middle cell being alive or dead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedMiddleCellDataset(torch.utils.data.Dataset):\n",
    "    _size = 256\n",
    "\n",
    "    def __init__(self, middle_cell):\n",
    "        if middle_cell not in [0, 1]:\n",
    "            raise ValueError(\"Middle cell value must be 0 or 1.\")\n",
    "        self._middle_cell = middle_cell\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= self._size:\n",
    "            raise IndexError()\n",
    "        # 12 -> \"0b1100\" -> \"1100\" -> \"00001100\"\n",
    "        idx_bin = bin(idx)[2:].rjust(8, \"0\")\n",
    "        idx_bin = idx_bin[:4] + str(self._middle_cell) + idx_bin[4:]\n",
    "        X = torch.tensor([float(ch) for ch in idx_bin], dtype=torch.float32).reshape(1, 3, 3) # (channels, width, height)\n",
    "        alive = X[0, 1, 1] > 0.5\n",
    "        alive_neighbours = torch.sum(X) - X[0, 1, 1]\n",
    "        next_alive = (alive and alive_neighbours > 1.5 and alive_neighbours < 3.5) or (not alive and alive_neighbours > 2.5 and alive_neighbours < 3.5)\n",
    "        y = torch.tensor([float(next_alive)], dtype=torch.float32).reshape(1, 1) # (width, height)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dataset = FixedMiddleCellDataset(1)\n",
    "neg_dataset = FixedMiddleCellDataset(0)\n",
    "pos_dataloader = DataLoader(pos_dataset)\n",
    "neg_dataloader = DataLoader(neg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all data through the first layer of the neural network and see the activation for the given feature.\n",
    "\n",
    "def get_pre_activations_for_feature(feature, dataloader):\n",
    "    pre_activations = np.zeros(len(dataloader))\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            z = model.conv0(X)\n",
    "            z = z.squeeze()[feature]\n",
    "            pre_activations[batch] = z.item()\n",
    "    return pre_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive pre-activations: mean 0.414, std 0.323, min -0.320 max 1.148 median 0.414\n",
      "Negative pre-activations: mean 0.515, std 0.323, min -0.219 max 1.248 median 0.515\n",
      "Ranges overlap: True\n",
      "Normalized difference of means: -0.22030\n"
     ]
    }
   ],
   "source": [
    "feature = 20\n",
    "\n",
    "pos_pre_activations = get_pre_activations_for_feature(feature, pos_dataloader)\n",
    "neg_pre_activations = get_pre_activations_for_feature(feature, neg_dataloader)\n",
    "\n",
    "pos_pre_activations\n",
    "print(\"Positive pre-activations:\",\n",
    "      f\"mean {np.mean(pos_pre_activations):.3f},\",\n",
    "      f\"std {np.std(pos_pre_activations):.3f},\",\n",
    "      f\"min {np.min(pos_pre_activations):.3f}\",\n",
    "      f\"max {np.max(pos_pre_activations):.3f}\",\n",
    "      f\"median {np.median(pos_pre_activations):.3f}\")\n",
    "print(f\"Negative pre-activations:\",\n",
    "      f\"mean {np.mean(neg_pre_activations):.3f},\",\n",
    "      f\"std {np.std(neg_pre_activations):.3f},\",\n",
    "      f\"min {np.min(neg_pre_activations):.3f}\",\n",
    "      f\"max {np.max(neg_pre_activations):.3f}\",\n",
    "      f\"median {np.median(neg_pre_activations):.3f}\")\n",
    "\n",
    "# Since the dataset represents the full population instead of just a sample, we can assess whether the feature distinguishes between\n",
    "# the two datasets by just checking if the ranges of pre-activations overlap.\n",
    "ranges_overlap = (np.max(pos_pre_activations) > np.min(neg_pre_activations)) and (np.max(neg_pre_activations) > np.min(pos_pre_activations))\n",
    "print(f\"Ranges overlap: {ranges_overlap}\")\n",
    "\n",
    "# Another way we can assess the difference is by looking at the difference of the means\n",
    "mean_diff = np.mean(pos_pre_activations) - np.mean(neg_pre_activations)\n",
    "# This should really be normalized by the standard deviation\n",
    "mean_diff_std = np.sqrt(np.var(pos_pre_activations) + np.var(neg_pre_activations))\n",
    "mean_diff_normalized  = mean_diff / mean_diff_std\n",
    "print(f\"Normalized difference of means: {mean_diff_normalized:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's get pre-activations for all features in a given layer. Then we can see which features seem to be differentiating most between the two datasets.\n",
    "\n",
    "def get_pre_activations(dataloader):\n",
    "    pre_activations = np.zeros((len(dataloader), model.conv0.out_channels))\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            z = model.conv0(X) # (batch_size, channels, width, height) -> (batch_size, channels, width, height)\n",
    "            z = z.squeeze() # (batch_size, channels, width, height) -> (channels,)\n",
    "            pre_activations[batch, :] = z\n",
    "    return pre_activations\n",
    "\n",
    "pos_pre_activations = get_pre_activations(pos_dataloader) # (dataset_size, num_features)\n",
    "neg_pre_activations = get_pre_activations(neg_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test using range overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each feature, let's assess range overlap\n",
    "\n",
    "def test_range_overlap(pos, neg):\n",
    "    return (pos.max(axis=0) >= neg.min(axis=0)) & (neg.max(axis=0) >= pos.min(axis=0))\n",
    "\n",
    "\n",
    "ranges_overlap = test_range_overlap(pos_pre_activations, neg_pre_activations)\n",
    "ranges_overlap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the ranges overlap! So maybe the concept isn't exemplified by a single feature in the conv0 layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test using normalized difference of means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.35055501, -1.18749087,  0.39031749, -0.18178185,  0.53735601,\n",
       "        0.28707581,  0.45817236, -0.3978403 , -0.83634738, -0.12782358,\n",
       "        1.33446187, -0.68430029,  1.03569632, -0.14038728, -0.10905208,\n",
       "       -0.81344565,  0.24017467,  0.69235158, -0.63207173,  0.39854949,\n",
       "       -0.22029624,  0.67174854, -0.62178556,  0.61221606, -0.61281918,\n",
       "       -0.50408767, -0.38026249,  0.13007052,  0.46328631,  0.14295915,\n",
       "        0.94649088,  0.68014044, -0.00449297,  0.04731425,  0.26528664,\n",
       "       -0.33205287,  0.01735685,  1.20233603, -0.28440147,  0.60779882,\n",
       "       -0.31448142, -0.85091064,  0.61773081, -0.61133225,  0.98860413,\n",
       "        0.68814836,  0.7716982 ,  0.33752691, -0.55906849,  0.36775185,\n",
       "        0.14225164,  0.43671175, -0.06118962, -0.30881532,  0.05641451,\n",
       "       -0.06657133,  0.00937756,  0.41167833, -0.04405811, -0.4941482 ,\n",
       "        0.72850119,  1.14242464,  0.02609398, -0.42680432,  0.17257551,\n",
       "        0.57126231,  0.00232297, -0.61229959, -0.66937132,  0.22221036,\n",
       "        0.1533143 ,  0.36994536,  0.50159937,  0.49213532, -0.77947051,\n",
       "        0.33254904, -0.71439383,  0.79675474,  1.05717933, -0.76874988,\n",
       "       -1.36903971, -0.03118386,  0.05121742,  0.38236563, -0.50961928])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mean_diff_normalized(pos, neg):\n",
    "    mean_diff = pos.mean(axis=0) - neg.mean(axis=0)\n",
    "    mean_diff_std = np.sqrt(pos.var(axis=0) + neg.var(axis=0))\n",
    "    mean_diff_normalized  = mean_diff / mean_diff_std\n",
    "    return mean_diff_normalized\n",
    "\n",
    "\n",
    "mean_diff_normalized = get_mean_diff_normalized(pos_pre_activations, neg_pre_activations)\n",
    "mean_diff_normalized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the activations are roughly normally distributed, we can run a two-tailed z-test on the normalized difference of means.\n",
    "We use a z-test rather than a t-test because we know the full dataset, so the std and variance are actually the population std and variance.\n",
    "\n",
    "For a two-tailed z-test at the 5% level, the critical value is 1.96. Let's see which normalized mean diffs are more extreme than +/- 1.96."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mean_diff_normalized > 1.96) | (mean_diff_normalized < -1.96)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welp, looks like the concept couldn't be extracted from any of the `conv0` features. Out of curiosity, what's the smallest significance level that would have made at least one test succeed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum z-value: -1.369\n",
      "Maximum z-value: 1.334\n"
     ]
    }
   ],
   "source": [
    "print(f\"Minimum z-value: {mean_diff_normalized.min():.3f}\")\n",
    "print(f\"Maximum z-value: {mean_diff_normalized.max():.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum z-value is -1.369 and the maximum is 1.334. Out of these, -1.369 is the most extreme and corresponds to a significance level of 17%. Not great."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-activation values\n",
    "\n",
    "Maybe the concept is more clearly represented in the post-activation values. Let's repeat the process for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_activations(dataloader):\n",
    "    post_activations = np.zeros((len(dataloader), model.conv0.out_channels))\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            a = F.relu(model.conv0(X)) # (batch_size, channels, width, height) -> (batch_size, channels, width, height)\n",
    "            a = a.squeeze() # (batch_size, channels, width, height) -> (channels,)\n",
    "            post_activations[batch, :] = a\n",
    "    return post_activations\n",
    "\n",
    "pos_post_activations = get_post_activations(pos_dataloader) # (dataset_size, num_features)\n",
    "neg_post_activations = get_post_activations(neg_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranges_overlap = test_range_overlap(pos_post_activations, neg_post_activations)\n",
    "ranges_overlap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, all the ranges overlap. This is to be expected. Relu is an increasing function, so it preserves weak inequalities. Range overlap is defined in terms of (weak) inequalities between the endpoints of the range. If ranges $ [a, b] $ and $ [c, d] $ overlap, this means $ b \\ge c $ and $ d \\ge a $. If we let $ a' = relu(a) $ etc., then $ b' \\ge c' $ and $ d' \\ge a' $, so the mapped ranges overlap too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akubi\\AppData\\Local\\Temp\\ipykernel_22900\\1777871862.py:4: RuntimeWarning: invalid value encountered in divide\n",
      "  mean_diff_normalized  = mean_diff / mean_diff_std\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.23683171, -1.14832957,  0.38838569, -0.17384463,  0.49229672,\n",
       "        0.27404148,  0.21782071, -0.14285725, -0.82830463, -0.11904263,\n",
       "        1.34079986, -0.59257538,  0.94553729, -0.13421474, -0.08376557,\n",
       "       -0.68746021,  0.20199103,  0.6868629 , -0.39566968,  0.39681848,\n",
       "       -0.21838428,  0.67088156, -0.16981416,  0.5653979 , -0.08868273,\n",
       "       -0.49532028, -0.36933328,  0.0983119 ,  0.46026895,  0.1420756 ,\n",
       "        0.94232762,  0.6800583 , -0.00465905,  0.03278738,  0.26288044,\n",
       "       -0.23377481,  0.01588153,  1.20730288, -0.14076101,  0.49609867,\n",
       "       -0.30384616, -0.76729917,  0.19509152, -0.37060805,  0.79466937,\n",
       "        0.68615106,  0.7628384 ,  0.30309226, -0.39586215,  0.35455994,\n",
       "        0.08984026,  0.39763355,         nan, -0.24108489,  0.04264589,\n",
       "       -0.05963881,  0.00879571,  0.4085628 , -0.02514456, -0.43708171,\n",
       "        0.5131398 ,  1.00241767,  0.02493692, -0.39736413,  0.16943566,\n",
       "        0.56779377,  0.00224058, -0.33743136, -0.65096573,  0.21896938,\n",
       "        0.15223755,  0.29607642,  0.40007525,  0.48999253, -0.61191473,\n",
       "        0.29803916, -0.68774753,  0.79755699,  1.06371193, -0.5132242 ,\n",
       "       -1.39359915, -0.02970064,  0.05069344,  0.25076174, -0.39066744])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_diff_normalized = get_mean_diff_normalized(pos_post_activations, neg_post_activations)\n",
    "mean_diff_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum z-value: -1.394\n",
      "Maximum z-value: 1.341\n"
     ]
    }
   ],
   "source": [
    "print(f\"Minimum z-value: {np.nanmin(mean_diff_normalized):.3f}\")\n",
    "print(f\"Maximum z-value: {np.nanmax(mean_diff_normalized):.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These z-values are pretty uninspiring, just like before."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher layer (`conv1`)\n",
    "\n",
    "There are two possible explanations for why we couldn't identify the \"middle cell alive\" feature in the `conv0` layer. Firstly, the model could have learnt the feature implicitly in some combination of the 85 channels in the `conv0` layer. Secondly, the model may have learnt the concept in the `conv1` layer instead of `conv0`. Let's test the second hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_activations_conv1(dataloader):\n",
    "    pre_activations = np.zeros((len(dataloader), model.conv1.out_channels))\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            z = F.relu(model.conv0(X)) # (batch_size, channels, width, height) -> (batch_size, channels, width, height)\n",
    "            z = model.conv1(z)\n",
    "            z = z.squeeze() # (batch_size, channels, width, height) -> (channels,)\n",
    "            pre_activations[batch, :] = z\n",
    "    return pre_activations\n",
    "\n",
    "pos_pre_activations = get_pre_activations_conv1(pos_dataloader) # (dataset_size, num_features)\n",
    "neg_pre_activations = get_pre_activations_conv1(neg_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranges_overlap = test_range_overlap(pos_pre_activations, neg_pre_activations)\n",
    "ranges_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akubi\\AppData\\Local\\Temp\\ipykernel_22900\\1777871862.py:4: RuntimeWarning: invalid value encountered in divide\n",
      "  mean_diff_normalized  = mean_diff / mean_diff_std\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.23683171, -1.14832957,  0.38838569, -0.17384463,  0.49229672,\n",
       "        0.27404148,  0.21782071, -0.14285725, -0.82830463, -0.11904263,\n",
       "        1.34079986, -0.59257538,  0.94553729, -0.13421474, -0.08376557,\n",
       "       -0.68746021,  0.20199103,  0.6868629 , -0.39566968,  0.39681848,\n",
       "       -0.21838428,  0.67088156, -0.16981416,  0.5653979 , -0.08868273,\n",
       "       -0.49532028, -0.36933328,  0.0983119 ,  0.46026895,  0.1420756 ,\n",
       "        0.94232762,  0.6800583 , -0.00465905,  0.03278738,  0.26288044,\n",
       "       -0.23377481,  0.01588153,  1.20730288, -0.14076101,  0.49609867,\n",
       "       -0.30384616, -0.76729917,  0.19509152, -0.37060805,  0.79466937,\n",
       "        0.68615106,  0.7628384 ,  0.30309226, -0.39586215,  0.35455994,\n",
       "        0.08984026,  0.39763355,         nan, -0.24108489,  0.04264589,\n",
       "       -0.05963881,  0.00879571,  0.4085628 , -0.02514456, -0.43708171,\n",
       "        0.5131398 ,  1.00241767,  0.02493692, -0.39736413,  0.16943566,\n",
       "        0.56779377,  0.00224058, -0.33743136, -0.65096573,  0.21896938,\n",
       "        0.15223755,  0.29607642,  0.40007525,  0.48999253, -0.61191473,\n",
       "        0.29803916, -0.68774753,  0.79755699,  1.06371193, -0.5132242 ,\n",
       "       -1.39359915, -0.02970064,  0.05069344,  0.25076174, -0.39066744])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_diff_normalized = get_mean_diff_normalized(pos_post_activations, neg_post_activations)\n",
    "mean_diff_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum z-value: -1.394\n",
      "Maximum z-value: 1.341\n"
     ]
    }
   ],
   "source": [
    "print(f\"Minimum z-value: {np.nanmin(mean_diff_normalized):.3f}\")\n",
    "print(f\"Maximum z-value: {np.nanmax(mean_diff_normalized):.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the ranges all overlap and the z-values are not extreme enough to conclude that any particular unit in the `conv1` layer reliably distinguishes between the positive and negative dataset. So, the `conv1` layer has also not explicitly learned the \"middle unit is alive\" concept."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In conclusion, there was no simple way to associate any particular unit in the network with the concept of the middle cell being alive. Intuitively, it seems clear that this concept should be relevant to the final prediction, but it seems that the network has learned a a different way of the predicting a cell's next state which doesn't use this concept directly.\n",
    "\n",
    "In later work, I could try and associate this concept to some linear combination of the activations of the `conv0` layer units. The idea would be to treat the 85 activations of the `conv0` units as independent variables and use linear regression to predict the current state of the middle cell. If this state can be reliably predicted, it means the concept exists in the network implicitly in some form. We could also measure how complex the encoding of this information is by looking at how many of the linear regression coefficients are significantly far from zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
